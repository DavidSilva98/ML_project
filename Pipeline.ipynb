{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile as zp\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score, GroupShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for file in [\"train.csv\", \"test.csv\"]:\n",
    "    with zp.ZipFile(\"./data.zip\") as myzip:\n",
    "        with myzip.open(file) as myfile:\n",
    "            df_list.append(pd.read_csv(myfile))\n",
    "            \n",
    "train_df, test_df = df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting variables\n",
    "train_df.rename({\"Parents or siblings infected\": \"Parents_Children_Infected\", \n",
    "                 \"Wife/Husband or children infected\": \"Partner_Siblings_Infected\"}, axis=1, inplace=True)\n",
    "test_df.rename({\"Parents or siblings infected\": \"Parents_Children_Infected\", \n",
    "                \"Wife/Husband or children infected\": \"Partner_Siblings_Infected\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.set_index(\"Patient_ID\", inplace=True)\n",
    "test_df.set_index(\"Patient_ID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('Deceased', axis=1)\n",
    "y_train = train_df['Deceased']\n",
    "X_test = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.City.fillna(value=\"Santa Fe\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.Medical_Tent = X_train.Medical_Tent.fillna(value=\"NK\")\n",
    "test_df.Medical_Tent = test_df.Medical_Tent.fillna(value=\"NK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Title'] = X_train['Name'].str.split('\\\\W', 1, expand=True)[0]\n",
    "test_df['Title'] = test_df['Name'].str.split('\\\\W', 1, expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Title_binary'] = X_train['Title'].apply(lambda x: 1 if x in [\"Master\",\"Miss\"] else 0)\n",
    "test_df['Title_binary'] = test_df['Title'].apply(lambda x: 1 if x in [\"Master\",\"Miss\"] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_function(X_data, columns_to_norm, columns_not_norm, scaler_type, scaler=None):\n",
    "    '''Function to apply normalization'''\n",
    "    \n",
    "    if scaler_type=='MinMaxScaler':\n",
    "        if scaler is None:\n",
    "            scaler = MinMaxScaler().fit(X_data[columns_to_norm])\n",
    "        X_scaled = scaler.transform(X_data[columns_to_norm])\n",
    "        \n",
    "    else:\n",
    "        if scaler is None:\n",
    "            scaler = StandardScaler().fit(X_data[columns_to_norm])\n",
    "        X_scaled = scaler.transform(X_data[columns_to_norm])\n",
    "        \n",
    "    X = np.append(X_data[columns_not_norm], X_scaled, axis=1)\n",
    "    X = pd.DataFrame(X, columns = columns_not_norm+columns_to_norm)\n",
    "\n",
    "    return X, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_type=\"MinMaxScaler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_imputer_birthday(X_train, test, scaler_type):\n",
    "    cols_drop = ['Family_Case_ID','Name','Medical_Expenses_Family','Medical_Tent','City']\n",
    "    columns_to_norm = ['Severity','Parents_Children_Infected','Partner_Siblings_Infected']\n",
    "    columns_not_norm=['Birthday_year','Title_binary']\n",
    "    \n",
    "    X_use = X_train.copy()\n",
    "    X_test = test.copy()\n",
    "    \n",
    "    fill_birthday_train, scaler = scale_function(X_use.drop(columns=cols_drop), columns_to_norm=columns_to_norm,\n",
    "                                                 columns_not_norm=columns_not_norm, scaler_type=scaler_type)\n",
    "    fill_birthday_test, scaler = scale_function(X_test.drop(columns=cols_drop), columns_to_norm=columns_to_norm,\n",
    "                                                 columns_not_norm=columns_not_norm, scaler_type=scaler_type)\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=3, weights='distance').fit(fill_birthday_train)\n",
    "    \n",
    "    X_imputted_birthday_train = imputer.transform(fill_birthday_train)\n",
    "    X_imputted_birthday_test = imputer.transform(fill_birthday_test)\n",
    "    \n",
    "    X_use.Birthday_year = list(pd.DataFrame(X_imputted_birthday_train).iloc[:,0]) # 0 is the index of Birthday_year\n",
    "    X_test.Birthday_year = list(pd.DataFrame(X_imputted_birthday_test).iloc[:,0]) # 0 is the index of Birthday_year\n",
    "    \n",
    "    return X_use, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, test_df = knn_imputer_birthday(X_train, test_df, scaler_type=scaler_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.Birthday_year = X_train.Birthday_year.round(0).astype(int)\n",
    "test_df.Birthday_year = test_df.Birthday_year.round(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Family_Case_ID</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Birthday_year</th>\n",
       "      <th>Parents_Children_Infected</th>\n",
       "      <th>Partner_Siblings_Infected</th>\n",
       "      <th>Medical_Expenses_Family</th>\n",
       "      <th>Medical_Tent</th>\n",
       "      <th>City</th>\n",
       "      <th>Title</th>\n",
       "      <th>Title_binary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patient_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4696</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss Linda Betty</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>225</td>\n",
       "      <td>NK</td>\n",
       "      <td>Santa Fe</td>\n",
       "      <td>Miss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21436</td>\n",
       "      <td>1</td>\n",
       "      <td>Ms. Ramona Elvira</td>\n",
       "      <td>1966</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1663</td>\n",
       "      <td>NK</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>Ms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7273</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Mario Vernon</td>\n",
       "      <td>1982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>221</td>\n",
       "      <td>NK</td>\n",
       "      <td>Santa Fe</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8226</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Hector Joe</td>\n",
       "      <td>1997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>NK</td>\n",
       "      <td>Santa Fe</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19689</td>\n",
       "      <td>3</td>\n",
       "      <td>Ms. Jennie Debra</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>222</td>\n",
       "      <td>NK</td>\n",
       "      <td>Santa Fe</td>\n",
       "      <td>Ms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Family_Case_ID  Severity               Name  Birthday_year  \\\n",
       "Patient_ID                                                               \n",
       "1                     4696         3   Miss Linda Betty           2005   \n",
       "2                    21436         1  Ms. Ramona Elvira           1966   \n",
       "3                     7273         3   Mr. Mario Vernon           1982   \n",
       "4                     8226         3     Mr. Hector Joe           1997   \n",
       "5                    19689         3   Ms. Jennie Debra           1994   \n",
       "\n",
       "            Parents_Children_Infected  Partner_Siblings_Infected  \\\n",
       "Patient_ID                                                         \n",
       "1                                   0                          0   \n",
       "2                                   0                          1   \n",
       "3                                   0                          0   \n",
       "4                                   0                          0   \n",
       "5                                   0                          0   \n",
       "\n",
       "            Medical_Expenses_Family Medical_Tent         City Title  \\\n",
       "Patient_ID                                                            \n",
       "1                               225           NK     Santa Fe  Miss   \n",
       "2                              1663           NK  Albuquerque    Ms   \n",
       "3                               221           NK     Santa Fe    Mr   \n",
       "4                               220           NK     Santa Fe    Mr   \n",
       "5                               222           NK     Santa Fe    Ms   \n",
       "\n",
       "            Title_binary  \n",
       "Patient_ID                \n",
       "1                      1  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "5                      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving parameter grid for specific Grid Search run\n",
    "class ComplexEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (StandardScaler, MinMaxScaler, OneHotEncoder, NeighborhoodComponentsAnalysis, SelectFromModel, DecisionTreeClassifier, PCA, np.ndarray, dict)):  # Include all classes that aren't serializable here\n",
    "            return str(obj)\n",
    "        # Let the base class default method raise the TypeError \n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "\n",
    "def gs_outputs(id_num, gscv_fitted, grid):\n",
    "    \"\"\"\n",
    "    Function that saves information of each grid-search.\n",
    "    \n",
    "    id_num: takes an id_number which identifies the grid-search\n",
    "    gscv_fitted: takes a fitted GridSearchCV object\n",
    "    grid: takes the grid used to fit the GridSearchCV object\n",
    "    \n",
    "    Returns:\n",
    "    top 20 configurations hyper-parameter presence graphic\n",
    "    (also outputs the \"logs\" of each grid-search to the output directory)\n",
    "    \"\"\"\n",
    "    # Saving parameter grid for specific Grid Search run\n",
    "    with open(\"./outputs/grids.txt\", \"a\") as file:\n",
    "        file.write(\"# {}------------------------------------------------------ #\\n\".format(id_num) + json.dumps(grid, cls=ComplexEncoder) + \"\\n\\n\")\n",
    "\n",
    "    # Saving cv_results for specific Grid Search run\n",
    "    score_summary = pd.DataFrame(gscv_fitted.cv_results_).sort_values(by=\"mean_test_score\", ascending=False)\n",
    "    score_summary.to_csv(\"./outputs/grid_search_results{}.csv\".format(id_num))\n",
    "    \n",
    "    # Assessing distribution of hyper-parameter values amongst top 20 models\n",
    "    sns.set()\n",
    "\n",
    "    # Features to plot\n",
    "    plot_features = list(map(lambda x: \"param_\" + x, {i for j in range(len(grid)) for i in grid[j].keys()}))\n",
    "    plot_df = score_summary.reset_index(drop=True).loc[:19, :].fillna(\"NaN\")\n",
    "\n",
    "    # figure and axes\n",
    "    fig, axes = plt.subplots(3, ceil(len(plot_features)/3), figsize=(23,13))\n",
    "\n",
    "    # plot data\n",
    "    for ax, x in zip(axes.flatten(), plot_features):\n",
    "        try:\n",
    "            sns.countplot(x=x, data=plot_df, ax=ax)\n",
    "        except TypeError:\n",
    "            sns.countplot(x=plot_df[x].apply(json.dumps), ax=ax)\n",
    "\n",
    "    plt.suptitle(\"Hyper-parameter presence on top 20 models\", y=0.95, fontsize=25)\n",
    "\n",
    "    plt.savefig(\"./outputs/grid_results{}.png\".format(id_num))\n",
    "    \n",
    "    # Model training - using best parameters to train model on entire data for submission\n",
    "    best_model = full_pipeline.set_params(**gscv_fitted.best_params_)\n",
    "    best_model.fit(X, y)  # Using all of the data to fit the model\n",
    "\n",
    "    # Predicting the \n",
    "    y_pred = best_model.predict(test_df)\n",
    "\n",
    "    # Submission\n",
    "    pd.DataFrame(data={\"Employee_ID\": test_df.index.to_list(), \"Churn_risk\": y_pred}).to_csv(\"./outputs/submission{}.csv\".format(id_num), index=False)\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def feature_importance(id_num, gscv_fitted, fimp_pre=True):\n",
    "    \"\"\"\n",
    "    Function that show feature importance plot and saves it to outputs folder\n",
    "    \n",
    "    id_num: takes an id_number which identifies the grid-search\n",
    "    gscv_fitted: takes a fitted GridSearchCV object\n",
    "    fimp_pre: whether to evaluate feature importance pre or pos model\n",
    "    \n",
    "    Returns:\n",
    "    feature importance graphic    \n",
    "    \"\"\"\n",
    "    if fimp_pre:\n",
    "        if (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"]) and (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"] != 'passthrough'):\n",
    "\n",
    "            # Get feature names\n",
    "            numeric_features = gscv_fitted.named_steps[\"prep\"].named_steps[\"join_features\"].transformers_[0][2]\n",
    "            categorical_features = gscv_fitted.named_steps[\"prep\"].named_steps[\"join_features\"].transformers_[1][2]\n",
    "            categorical_features = list(gscv_fitted.named_steps[\"prep\"].named_steps[\"join_features\"].named_transformers_[\"categorical_pipeline\"].named_steps[\"one_hot_encoder\"].get_feature_names(categorical_features))\n",
    "            feature_names = metric_features + categorical_features\n",
    "\n",
    "            # Get feature importances\n",
    "            if hasattr(gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].estimator_, \"feature_importances_\"):  # Check if features_importances_ attribute exists. Necessary when there's tree selectors\n",
    "                if len(gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].estimator_.feature_importances_.shape) != 2: \n",
    "                    feature_importances = pd.DataFrame(gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].estimator_.feature_importances_, index=feature_names).T\n",
    "                else:\n",
    "                    feature_importances = pd.DataFrame(gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].estimator_.feature_importances_, index=gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].estimator_.classes_, columns=feature_names)\n",
    "            else:\n",
    "                if len(gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].estimator_.coef_.shape) != 2: \n",
    "                    feature_importances = pd.DataFrame(gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].estimator_.coef_, index=feature_names).T\n",
    "                else:\n",
    "                    feature_importances = pd.DataFrame(gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].estimator_.coef_, index=gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].estimator_.classes_, columns=feature_names)\n",
    "\n",
    "            colors = feature_importances.applymap(lambda x: \"tab:red\" if x < 0 else \"tab:blue\")  # Get positive and negative colors\n",
    "            threshold = gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].threshold_  # Get SelectFromModel threshold value \n",
    "\n",
    "            sns.set()\n",
    "            if feature_importances.shape[0] == 3:\n",
    "                fig, axes = plt.subplots(3, 1, figsize=(23,12))\n",
    "                blue_patch = mpatches.Patch(color='tab:blue', label='Positive')\n",
    "                red_patch = mpatches.Patch(color='tab:red', label='Negative')\n",
    "                threshold_line = mlines.Line2D([], [], linestyle=\"dashed\", color='black', label='Threshold')\n",
    "\n",
    "                for i, ax in enumerate(axes.flatten()):\n",
    "                    ax.bar(x=feature_names, height=abs(feature_importances.iloc[i]), color=colors.iloc[i])\n",
    "                    ax.hlines(threshold, -0.5, 18.5, linestyles=\"dashed\")\n",
    "                    ax.set_xticklabels(feature_names, rotation = 30, ha=\"right\")\n",
    "                    ax.set_title(\"Class {} Feature Importance\".format(feature_importances.index[i]), fontsize=18)\n",
    "                    ax.legend(handles=[blue_patch, red_patch, threshold_line], fontsize=12)\n",
    "\n",
    "                plt.subplots_adjust(hspace=0.6)\n",
    "                plt.savefig(\"./outputs/feature_importance{}.png\".format(id_num))\n",
    "                return plt.show()\n",
    "            else: \n",
    "                fig = plt.figure(figsize=(23,6))\n",
    "                blue_patch = mpatches.Patch(color='tab:blue', label='Positive')\n",
    "                red_patch = mpatches.Patch(color='tab:red', label='Negative')\n",
    "                threshold_line = mlines.Line2D([], [], linestyle=\"dashed\", color='black', label='Threshold')\n",
    "\n",
    "                plt.bar(x=feature_names, height=abs(feature_importances.iloc[0]), color=colors.iloc[0])\n",
    "                plt.hlines(threshold, -0.5, len(feature_names) - 0.5, linestyles=\"dashed\")\n",
    "                plt.xticks(rotation=30, ha=\"right\")\n",
    "                plt.title(\"Pre Feature Importance\", fontsize=18)\n",
    "                plt.legend(handles=[blue_patch, red_patch, threshold_line], fontsize=12)\n",
    "\n",
    "                plt.savefig(\"./outputs/feature_importance_pre{}.png\".format(id_num))\n",
    "                return plt.show()\n",
    "        else:\n",
    "            print(\"No feature selection was done!\")\n",
    "    else:\n",
    "        if (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect2\"]) and (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect2\"] != 'passthrough'):\n",
    "            # Obtaining Component Names\n",
    "            feature_names = [\"PC\" + str(i) for i in range(1, best_model.named_steps[\"prep\"].named_steps[\"fselect2\"].n_components + 1)]\n",
    "        else:  \n",
    "            # Get feature names\n",
    "            numeric_features = gscv_fitted.named_steps[\"prep\"].named_steps[\"join_features\"].transformers_[0][2]\n",
    "            categorical_features = gscv_fitted.named_steps[\"prep\"].named_steps[\"join_features\"].transformers_[1][2]\n",
    "            categorical_features = list(gscv_fitted.named_steps[\"prep\"].named_steps[\"join_features\"].named_transformers_[\"categorical_pipeline\"].named_steps[\"one_hot_encoder\"].get_feature_names(categorical_features))\n",
    "            feature_names = metric_features + categorical_features\n",
    "\n",
    "            # Get filtered feature names\n",
    "            if (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"]) and (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"] != 'passthrough'):\n",
    "                feature_names = np.array(feature_names)[gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].get_support()]\n",
    "\n",
    "        # Get feature importances\n",
    "        if len(gscv_fitted.named_steps[\"model\"].feature_importances_.shape) != 2: \n",
    "            feature_importances = pd.DataFrame(gscv_fitted.named_steps[\"model\"].feature_importances_, index=feature_names).T\n",
    "        else:\n",
    "            feature_importances = pd.DataFrame(gscv_fitted.named_steps[\"model\"].feature_importances_, index=gscv_fitted.named_steps[\"model\"].classes_, columns=feature_names)\n",
    "        \n",
    "        colors = feature_importances.applymap(lambda x: \"tab:red\" if x < 0 else \"tab:blue\")  # Get positive and negative colors\n",
    "\n",
    "        sns.set()\n",
    "        fig = plt.figure(figsize=(23,6))\n",
    "        blue_patch = mpatches.Patch(color='tab:blue', label='Positive')\n",
    "        red_patch = mpatches.Patch(color='tab:red', label='Negative')\n",
    "\n",
    "        plt.bar(x=feature_names, height=abs(feature_importances.iloc[0]), color=colors.iloc[0])\n",
    "        plt.xticks(rotation=30, ha=\"right\")\n",
    "        plt.title(\"Pos Feature Importance\", fontsize=18)\n",
    "        plt.legend(handles=[blue_patch, red_patch], fontsize=12)\n",
    "\n",
    "        plt.savefig(\"./outputs/feature_importance_pos{}.png\".format(id_num))\n",
    "        return plt.show()\n",
    "        \n",
    "\n",
    "def plot_tree(id_num, gscv_fitted):\n",
    "    \"\"\"\n",
    "    Function that shows the decision tree graph and saves it as png\n",
    "    \n",
    "    id_num: takes an id_number which identifies the grid-search\n",
    "    gscv_fitted: takes a fitted GridSearchCV object\n",
    "    \n",
    "    Returns:\n",
    "    decision tree graphic\n",
    "    \"\"\"\n",
    "    if (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect2\"]) and (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect2\"] != 'passthrough'):\n",
    "        # Obtaining Component Names\n",
    "        feature_names = [\"PC\" + str(i) for i in range(1, best_model.named_steps[\"prep\"].named_steps[\"fselect2\"].n_components + 1)]\n",
    "    else:  \n",
    "        # Get feature names\n",
    "        numeric_features = gscv_fitted.named_steps[\"prep\"].named_steps[\"join_features\"].transformers_[0][2]\n",
    "        categorical_features = gscv_fitted.named_steps[\"prep\"].named_steps[\"join_features\"].transformers_[1][2]\n",
    "        categorical_features = list(gscv_fitted.named_steps[\"prep\"].named_steps[\"join_features\"].named_transformers_[\"categorical_pipeline\"].named_steps[\"one_hot_encoder\"].get_feature_names(categorical_features))\n",
    "        feature_names = metric_features + categorical_features\n",
    "\n",
    "        # Get filtered feature names\n",
    "        if (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"]) and (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"] != 'passthrough'):\n",
    "            feature_names = np.array(feature_names)[gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].get_support()]\n",
    "    \n",
    "    # Set PATH variable to locate graphviz executables\n",
    "    if os.environ['PATH'].split(\";\")[-1] != 'C:\\\\Users\\\\davids\\\\Anaconda3\\\\envs\\\\ml\\\\Library\\\\bin\\\\graphviz':\n",
    "        os.environ['PATH'] = os.environ['PATH'] + ';' + os.environ['CONDA_PREFIX'] + r\"\\Library\\bin\\graphviz\"\n",
    "    \n",
    "    # Export a decision tree in DOT format\n",
    "    dot_data = export_graphviz(gscv_fitted.named_steps[\"model\"],\n",
    "                               feature_names=feature_names,  \n",
    "                               class_names=gscv_fitted.named_steps[\"model\"].classes_,\n",
    "                               filled=True,\n",
    "                               rounded=True)\n",
    "    \n",
    "    pydot_graph = pydotplus.graph_from_dot_data(dot_data)  # load graph in dot format\n",
    "    pydot_graph.set_size('\"50,50\"')  # set size of graph figure\n",
    "    pydot_graph.write_png(\"./outputs/decision_tree{}.png\".format(id_num))  # output graph as png\n",
    "    return graphviz.Source(pydot_graph.to_string())\n",
    "\n",
    "\n",
    "def effective_alpha_analysis(id_num, gscv_fitted, X_train, y_train, max_alpha, min_alpha=0):\n",
    "    \"\"\"\n",
    "    Function that analyses the alphas of a decision tree and plots the cross-validated mean alpha performance of default trees.\n",
    "    \n",
    "    id_num: takes an id_number which identifies the grid-search\n",
    "    gscv_fitted: takes a fitted GridSearchCV object\n",
    "    X_train: raw train data\n",
    "    y_train: train data labels\n",
    "    max_alpha: max alpha consider in cross-validation\n",
    "    min_alpha: min alpha consider in cross-validation\n",
    "    \n",
    "    Returns:\n",
    "    alpha analysis graphic\n",
    "    \"\"\"\n",
    "    original_model = clone(gscv_fitted.named_steps[\"model\"])\n",
    "    analyze_model = clone(gscv_fitted.named_steps[\"model\"].set_params(min_weight_fraction_leaf=0.0, max_depth=None))\n",
    "\n",
    "    # Transforming X_train based on fitted preparation pipeline\n",
    "    X_train_trans = gscv_fitted.named_steps[\"prep\"].transform(X_train)\n",
    "\n",
    "    # Producing cost_complexity_pruning_path of default tree on X_train_trans\n",
    "    path = analyze_model.cost_complexity_pruning_path(X_train_trans, y_train)\n",
    "    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "    # Defininf a default tree for each ccp_alpha value found\n",
    "    ccp_alphas_delim = ccp_alphas[(ccp_alphas >= min_alpha) & (ccp_alphas <= max_alpha)]\n",
    "    trees = [clone(analyze_model.set_params(ccp_alpha=ccp_alpha)) for ccp_alpha in ccp_alphas_delim]\n",
    "\n",
    "    # Producing 5-fold cv f1-scores for each tree on X_train_trans\n",
    "    cv_scores = np.array([cross_val_score(tree, X_train_trans, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=1), scoring='f1_micro', n_jobs=-1) for tree in trees])\n",
    "    cv_scores_mean, cv_scores_std = cv_scores.mean(axis=1), cv_scores.std(axis=1)\n",
    "    cv_scores_ci95 = 1.96 * cv_scores_std / cv_scores_mean\n",
    "    global_optimum = [ccp_alphas_delim[np.argmax(cv_scores_mean)], np.max(cv_scores_mean)]\n",
    "    original_cv_score = cross_val_score(original_model, X_train_trans, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=1), scoring='f1_micro', n_jobs=-1).mean()\n",
    "    \n",
    "    sns.set()\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(23, 8))\n",
    "\n",
    "    # Plot Total Impurity vs effective alpha for training set\n",
    "    ax1.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "    ax1.set_xlabel(\"effective alpha\", fontsize=14)\n",
    "    ax1.set_ylabel(\"total impurity of leaves\", fontsize=14)\n",
    "    ax1.set_title(\"Total Impurity vs effective alpha for training set\", fontsize=18)\n",
    "\n",
    "    # Plot F1-score vs alpha for cross-validation score\n",
    "    ax2.plot(ccp_alphas_delim, cv_scores_mean, marker='o', label=\"AVG Scores\", drawstyle=\"steps-post\")\n",
    "    ax2.plot(global_optimum[0], global_optimum[1], 'o', label=\"G.O.: [{0:.6f}, {1:.3f}]\".format(global_optimum[0], global_optimum[1]))\n",
    "    ax2.fill_between(ccp_alphas_delim, (cv_scores_mean - cv_scores_ci95), (cv_scores_mean + cv_scores_ci95), color='b', alpha=.1)\n",
    "    ax2.hlines(original_cv_score, min_alpha, max_alpha, linestyles=\"dashed\", label=\"Original Model\")\n",
    "    ax2.set_xlim(left=min_alpha, right=max_alpha)\n",
    "    ax2.set_xlabel(\"alpha\", fontsize=14)\n",
    "    ax2.set_ylabel(\"f1-score\", fontsize=14)\n",
    "    ax2.set_title(\"F1-score vs alpha for cross-validation score\", fontsize=18)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.savefig(\"./outputs/alpha_analysis{}.png\".format(id_num))\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell holds all the Custom Transformers we designed to pipeline the raw data to the model\n",
    "class PrepImpute(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Creates Title\n",
    "    Creates Title_binary\n",
    "    Fills Medical_Tent with \"NK\"\n",
    "    Imputes City with most frequent value\n",
    "    \"\"\"\n",
    "    #Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.city_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        \n",
    "    #Return self nothing else to do here\n",
    "    def fit(self, X, y=None):\n",
    "        self.city_imputer.fit(X[[\"City\"]])\n",
    "        return self\n",
    "    \n",
    "    #Transformer method we wrote for this transformer \n",
    "    def transform(self, X):\n",
    "        \n",
    "        X2 = X.copy()\n",
    "        \n",
    "        # CREATE NEW VARIABLES\n",
    "        X2['Title'] = X2['Name'].str.split('\\\\W', 1, expand=True)[0]\n",
    "        X2['Title_binary'] = X2['Title'].apply(lambda x: 1 if x in [\"Master\",\"Miss\"] else 0)\n",
    "        X2[\"Medical_Tent\"] = X2[\"Medical_Tent\"].fillna(value=\"NK\")\n",
    "        X2['City'] = self.city_imputer.transform(X2[[\"City\"]])\n",
    "        \n",
    "        return X2 \n",
    "    \n",
    "\n",
    "class KNNImputerScaled(KNNImputer):\n",
    "    \"\"\"\n",
    "    KNNImputer subclass that scales variables before applying imputation\n",
    "    Impute Birthday_year using scaled and non-scaled variables with KNNImputer\n",
    "    \"\"\"\n",
    "    def __init__(self, missing_values=np.nan, n_neighbors=5, weights='uniform', metric='nan_euclidean', copy=True, add_indicator=False,\n",
    "                 columns_to_norm=['Severity','Parents_Children_Infected','Partner_Siblings_Infected'], columns_not_to_norm=['Birthday_year','Title_binary'],\n",
    "                 scaler=MinMaxScaler(), **scaler_args):\n",
    "        super().__init__(missing_values=missing_values, \n",
    "                         n_neighbors=n_neighbors, \n",
    "                         weights=weights,\n",
    "                         metric=metric,\n",
    "                         copy=copy, \n",
    "                         add_indicator=add_indicator)\n",
    "        self.scaler = scaler.set_params(**scaler_args)\n",
    "        self.columns_to_norm = columns_to_norm\n",
    "        self.columns_not_to_norm = columns_not_to_norm\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X_scaled = self.scaler.fit_transform(X[self.columns_to_norm])\n",
    "        X_trans = np.concatenate([X_scaled, X[self.columns_not_to_norm]], axis=1)\n",
    "        super().fit(X_trans)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_scaled = self.scaler.transform(X[self.columns_to_norm])\n",
    "        X_trans = np.concatenate([X_scaled, X[self.columns_not_to_norm]], axis=1)\n",
    "        X_imputed_scaled = pd.DataFrame(super().transform(X_trans), index=X.index, columns=self.columns_to_norm + self.columns_not_to_norm)\n",
    "        X_imputed = X.copy()\n",
    "        X_imputed[\"Birthday_year\"] = X_imputed_scaled[\"Birthday_year\"].round(0).astype(int)\n",
    "        \n",
    "        return X_imputed\n",
    "    \n",
    "    \n",
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Creates Age\n",
    "    Creates Gender\n",
    "    Ceates Parents_Children_Infected_Binary\n",
    "    Creates Partner_Siblings_Infected_Binary\n",
    "    Creates Medical_Tent_Binary\n",
    "    Creates Family_Infected\n",
    "    Creates Family_Infected_Binary\n",
    "    Creates Family_Deceased\n",
    "    Creates Family_Deceased_Reduced\n",
    "    Creates Medical_Expenses_Individual\n",
    "    \"\"\"\n",
    "    #Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        pass    \n",
    "        \n",
    "    #Return self nothing else to do here\n",
    "    def fit(self, X, y):\n",
    "        self.familycount_ = X.Family_Case_ID.value_counts().to_dict()\n",
    "        X_train_w_dec = pd.concat([X, y], axis=1)\n",
    "        self.deceasedfamcount_ = X_train_w_dec.loc[X_train_w_dec['Deceased']==1].Family_Case_ID.value_counts().to_dict()\n",
    "        return self\n",
    "    \n",
    "    #Transformer method we wrote for this transformer \n",
    "    def transform(self, X):\n",
    "        X2 = X.copy()\n",
    "        # CREATE NEW VARIABLES\n",
    "        X2['Age'] = X2['Birthday_year'].map(lambda x: 2020 - x)\n",
    "        X2['Gender'] = X2['Title'].map(lambda x: \"Male\" if x in [\"Mr\", \"Master\"] else \"Female\")\n",
    "        X2['Parents_Children_Infected_Binary'] = X2['Parents_Children_Infected'].map(lambda x: 0 if x==0 else 1)  \n",
    "        X2['Partner_Siblings_Infected_Binary'] = X2['Partner_Siblings_Infected'].map(lambda x: 0 if x==0 else 1)\n",
    "        X2[\"Medical_Tent_Binary\"] = X2[\"Medical_Tent\"].map(lambda x: x if x == \"NK\" else \"K\")\n",
    "        #X2[\"Pediatric_Binary\"] = X2[\"Age\"].map(lambda x: 1 if x < 18 else 0)\n",
    "        #X2[\"3rd_Age_Binary\"] = X2[\"Age\"].map(lambda x: 1 if x >= 65 else 0)\n",
    "        X2[\"Family_Infected\"] = X2[\"Family_Case_ID\"].map(self.familycount_).fillna(1)\n",
    "        X2[\"Family_Infected_Binary\"] = X2[\"Family_Infected\"].map(lambda x: 0 if x<=1 else 1)\n",
    "        X2[\"Family_Deceased\"] = (X2[\"Family_Case_ID\"].map(self.deceasedfamcount_) - 1).fillna(0)\n",
    "        X2[\"Family_Deceased_Reduced\"] = X2['Family_Deceased'].map(lambda x: x if x==0 else (2 if x>=3 else 1))\n",
    "        #X2[\"Dead_infected_ratio_family\"] = X2[\"Family_Deceased\"] / X2[\"Family_Infected\"]\n",
    "        X2[\"Medical_Expenses_Individual\"] = X2[\"Medical_Expenses_Family\"] / X2[\"Family_Infected\"]\n",
    "        # FURTHER TRANSFORMATIONS\n",
    "        X2[\"Medical_Expenses_Individual\"][X2[\"Medical_Expenses_Individual\"] > 3000] = 3000\n",
    "        \n",
    "        return X2 \n",
    "    \n",
    "    \n",
    "class Transformations(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transforms Medical_Expenses_Individual by applying Box-Cox power transformation\n",
    "    Drops unnecessary variables\n",
    "    \"\"\"\n",
    "    #Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    #Return self nothing else to do here\n",
    "    def fit(self, X, y=None):\n",
    "        self.lambda_ = boxcox(X[\"Medical_Expenses_Individual\"] + 1)[1]\n",
    "        return self\n",
    "\n",
    "    #Transformer method we wrote for this transformer \n",
    "    def transform(self, X):\n",
    "        X[\"Medical_Expenses_Individual\"] = boxcox(X[\"Medical_Expenses_Individual\"] + 1, alpha=self.lambda_)[0]\n",
    "        # DROP INTERMEDIATE VARIABLES\n",
    "        X = X.drop([\"Family_Case_ID\", \"Name\", \"Birthday_year\", \"Title\", \"Title_binary\", \"Partner_Siblings_Infected\", \n",
    "                    \"Medical_Tent\", \"Parents_Children_Infected\", \"Partner_Siblings_Infected_Binary\", \n",
    "                    \"Parents_Children_Infected_Binary\", \"Family_Deceased\", \"Family_Infected\", \"Medical_Expenses_Family\"], axis=1)\n",
    "        \n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pipe = train_df.drop('Deceased', axis=1)\n",
    "y_train_pipe = train_df['Deceased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pi = PrepImpute()\n",
    "# X_trans = pi.fit_transform(X_train_pipe)\n",
    "# X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knnimp = KNNImputerScaled(n_neighbors=3, weights='distance', scaler=MinMaxScaler)\n",
    "# knnimp.fit(X_trans)\n",
    "# X_scaled = knnimp.transform(X_trans)\n",
    "# X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking that KNNImputerScaled gets the same output as the KNNImputer used outside pipeline\n",
    "# (X_train.loc[train_df[\"Birthday_year\"].isna(), \"Birthday_year\"] == X_scaled.loc[train_df[\"Birthday_year\"].isna(), \"Birthday_year\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxcox1, lambd = boxcox(x=X_train[\"Medical_Expenses_Family\"]+1)\n",
    "# boxcox2 = boxcox(x=(X_train[\"Medical_Expenses_Family\"]+1), alpha=lambd)[0]\n",
    "# boxcox1, boxcox2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxcox3 = boxcox(x=test_df[\"Medical_Expenses_Family\"]+1, alpha=lambd)[0]\n",
    "# boxcox3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Pipeline\n",
    "clean_pipeline = Pipeline([\n",
    "    ('prepimpute', PrepImpute()),\n",
    "    ('knnimputer', KNNImputerScaled()),  # tune: n_neighbors, weights, scaler\n",
    "    ('engineering', FeatureEngineering()),\n",
    "    ('transformation', Transformations())\n",
    "])\n",
    "\n",
    "# Spliting features\n",
    "scale_columns = [\"Severity\", \"Age\", \"Family_Deceased_Reduced\", \"Medical_Expenses_Individual\"]\n",
    "ohe_columns = [\"City\", \"Gender\", \"Medical_Tent_Binary\", \"Family_Infected_Binary\"]\n",
    "\n",
    "# Combining features and applying different transformations\n",
    "join_pipeline = ColumnTransformer([('scaler', \"passthrough\", scale_columns),\n",
    "                                   ('ohe', OneHotEncoder(sparse=False), ohe_columns)])  # tune: drop\n",
    "\n",
    "# Feature Selection\n",
    "fselect1 = SelectFromModel(LogisticRegression(penalty=\"l1\", max_iter=400, multi_class=\"multinomial\", solver=\"saga\", n_jobs=-1, random_state=1))  # tune: estimator__C\n",
    "fselect2 = SelectFromModel(LogisticRegression(penalty=\"l2\", max_iter=400,  multi_class=\"multinomial\", solver=\"saga\", n_jobs=-1, random_state=1))  # tune: estimator__C, threshold\n",
    "dimred1 = NeighborhoodComponentsAnalysis(max_iter=25, tol=0.005, random_state=1)  # tune: n_components, tol=0.005 (it takes to long)\n",
    "\n",
    "# Full Preprocessing Pipeline\n",
    "prep_pipeline = Pipeline([\n",
    "    (\"clean\", clean_pipeline),\n",
    "    (\"join\", join_pipeline),\n",
    "    (\"fselect\", \"passthrough\"),\n",
    "    (\"dimred\", \"passthrough\")\n",
    "])\n",
    "\n",
    "# Model\n",
    "lr = LogisticRegression(max_iter=400, random_state=1, n_jobs=-1)\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "# Full Model Pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    (\"prep\", prep_pipeline),\n",
    "    (\"model\", \"passthrough\")\n",
    "])\n",
    "\n",
    "grid = [\n",
    "    {\"prep__clean__knnimputer__n_neighbors\": [3, 5, 9, 17],\n",
    "     \"prep__clean__knnimputer__weights\": [\"uniform\", \"distance\"],\n",
    "     \"prep__clean__knnimputer__scaler\": [StandardScaler(), MinMaxScaler()],\n",
    "     \"prep__join__scaler\": [StandardScaler(), MinMaxScaler()],\n",
    "     \"prep__join__ohe__drop\": [None, \"first\"],  # define categories to drop\n",
    "     \"prep__fselect\": [None],\n",
    "     \"prep__dimred\": [None],\n",
    "     \"model\": [knn],\n",
    "     \"model__n_neighbors\": [5, 9, 17],\n",
    "     \"model__weights\": [\"uniform\", \"distance\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_trans = pd.DataFrame(prep_pipeline.fit_transform(X_train_pipe, y_train_pipe), index=X_train_pipe.index, columns=scale_columns + list(prep_pipeline.named_steps[\"join\"].named_transformers_[\"ohe\"].get_feature_names(ohe_columns)))\n",
    "# X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Insert GridSearch ID number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1920 out of 1920 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=1, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('prep',\n",
       "                                        Pipeline(memory=None,\n",
       "                                                 steps=[('clean',\n",
       "                                                         Pipeline(memory=None,\n",
       "                                                                  steps=[('prepimpute',\n",
       "                                                                          PrepImpute()),\n",
       "                                                                         ('knnimputer',\n",
       "                                                                          KNNImputerScaled(add_indicator=False,\n",
       "                                                                                           columns_not_to_norm=['Birthday_year',\n",
       "                                                                                                                'Title_binary'],\n",
       "                                                                                           columns_to_norm=['S...\n",
       "                          'prep__clean__knnimputer__weights': ['uniform',\n",
       "                                                               'distance'],\n",
       "                          'prep__dimred': [None], 'prep__fselect': [None],\n",
       "                          'prep__join__ohe__drop': [None, 'first'],\n",
       "                          'prep__join__scaler': [StandardScaler(copy=True,\n",
       "                                                                with_mean=True,\n",
       "                                                                with_std=True),\n",
       "                                                 MinMaxScaler(copy=True,\n",
       "                                                              feature_range=(0,\n",
       "                                                                             1))]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instatiating GridSearch\n",
    "gscv = GridSearchCV(full_pipeline, grid, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=1), scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Get ID of grid search\n",
    "id_num = input(\"Insert GridSearch ID number: \")\n",
    "\n",
    "# Grid Search and model training\n",
    "gscv.fit(X_train_pipe, y_train_pipe)\n",
    "\n",
    "# Obtain outputs from Grid Search\n",
    "# gs_outputs(id_num, gscv, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8122222222222222"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                      metric_params=None, n_jobs=-1, n_neighbors=17, p=2,\n",
       "                      weights='distance'),\n",
       " 'model__n_neighbors': 17,\n",
       " 'model__weights': 'distance',\n",
       " 'prep__clean__knnimputer__n_neighbors': 17,\n",
       " 'prep__clean__knnimputer__scaler': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'prep__clean__knnimputer__weights': 'distance',\n",
       " 'prep__dimred': None,\n",
       " 'prep__fselect': None,\n",
       " 'prep__join__ohe__drop': None,\n",
       " 'prep__join__scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Family_Case_ID</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Birthday_year</th>\n",
       "      <th>Parents_Children_Infected</th>\n",
       "      <th>Partner_Siblings_Infected</th>\n",
       "      <th>Medical_Expenses_Family</th>\n",
       "      <th>Medical_Tent</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patient_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>49242</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Jody Pedro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santa Fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>10400</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Kevin Brent</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santa Fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>10795</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Frankie Cary</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Albuquerque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>62440</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Rick Pete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Albuquerque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>81311</td>\n",
       "      <td>2</td>\n",
       "      <td>Mr. Matthew Erick</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santa Fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>110522</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Luther Rogelio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santa Fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>118768</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Emanuel Ruben</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Albuquerque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>86158</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. Misty Camille</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3830</td>\n",
       "      <td>C</td>\n",
       "      <td>Albuquerque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>18523</td>\n",
       "      <td>3</td>\n",
       "      <td>Master Gustavo Jordan</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santa Fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>19893</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. Minnie Sheryl</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1774</td>\n",
       "      <td>D</td>\n",
       "      <td>Albuquerque</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Family_Case_ID  Severity                   Name  Birthday_year  \\\n",
       "Patient_ID                                                                   \n",
       "901                  49242         3         Mr. Jody Pedro            NaN   \n",
       "902                  10400         3        Mr. Kevin Brent         1988.0   \n",
       "903                  10795         3       Mr. Frankie Cary         1981.0   \n",
       "904                  62440         3          Mr. Rick Pete            NaN   \n",
       "905                  81311         2      Mr. Matthew Erick         1996.0   \n",
       "...                    ...       ...                    ...            ...   \n",
       "1296                110522         3     Mr. Luther Rogelio            NaN   \n",
       "1297                118768         3      Mr. Emanuel Ruben            NaN   \n",
       "1298                 86158         1     Mrs. Misty Camille         1994.0   \n",
       "1299                 18523         3  Master Gustavo Jordan         2007.0   \n",
       "1300                 19893         1     Mrs. Minnie Sheryl         1975.0   \n",
       "\n",
       "            Parents_Children_Infected  Partner_Siblings_Infected  \\\n",
       "Patient_ID                                                         \n",
       "901                                 0                          0   \n",
       "902                                 0                          0   \n",
       "903                                 1                          0   \n",
       "904                                 0                          1   \n",
       "905                                 0                          0   \n",
       "...                               ...                        ...   \n",
       "1296                                0                          0   \n",
       "1297                                0                          0   \n",
       "1298                                0                          1   \n",
       "1299                                2                          0   \n",
       "1300                                1                          0   \n",
       "\n",
       "            Medical_Expenses_Family Medical_Tent         City  \n",
       "Patient_ID                                                     \n",
       "901                             203          NaN     Santa Fe  \n",
       "902                             631          NaN     Santa Fe  \n",
       "903                             376          NaN  Albuquerque  \n",
       "904                             405          NaN  Albuquerque  \n",
       "905                             378          NaN     Santa Fe  \n",
       "...                             ...          ...          ...  \n",
       "1296                            221          NaN     Santa Fe  \n",
       "1297                            202          NaN  Albuquerque  \n",
       "1298                           3830            C  Albuquerque  \n",
       "1299                            567          NaN     Santa Fe  \n",
       "1300                           1774            D  Albuquerque  \n",
       "\n",
       "[400 rows x 9 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gscv.predict(X_test), index=X_test.index, columns=[\"Deceased\"]).to_csv(\"./outputs/submission1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
