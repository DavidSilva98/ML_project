{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile as zp\n",
    "from math import ceil\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score, GroupShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# from sklearn import set_config\n",
    "# set_config(display='diagram')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for file in [\"train.csv\", \"test.csv\"]:\n",
    "    with zp.ZipFile(\"./data.zip\") as myzip:\n",
    "        with myzip.open(file) as myfile:\n",
    "            df_list.append(pd.read_csv(myfile))\n",
    "            \n",
    "train_df, test_df = df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting variables\n",
    "train_df.rename({\"Parents or siblings infected\": \"Parents_Children_Infected\", \n",
    "                 \"Wife/Husband or children infected\": \"Partner_Siblings_Infected\"}, axis=1, inplace=True)\n",
    "test_df.rename({\"Parents or siblings infected\": \"Parents_Children_Infected\", \n",
    "                \"Wife/Husband or children infected\": \"Partner_Siblings_Infected\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.set_index(\"Patient_ID\", inplace=True)\n",
    "test_df.set_index(\"Patient_ID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('Deceased', axis=1)\n",
    "y_train = train_df['Deceased']\n",
    "X_test = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving parameter grid for specific Grid Search run\n",
    "class ComplexEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (StandardScaler, MinMaxScaler, OneHotEncoder, RandomForestClassifier, KNeighborsClassifier, NeighborhoodComponentsAnalysis, LogisticRegression, SelectFromModel, np.ndarray, dict)):  # Include all classes that aren't serializable here\n",
    "            return str(obj)\n",
    "        # Let the base class default method raise the TypeError \n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "\n",
    "def gs_outputs(id_num, gscv_fitted, grid):\n",
    "    \"\"\"\n",
    "    Function that saves information of each grid-search.\n",
    "    \n",
    "    id_num: takes an id_number which identifies the grid-search\n",
    "    gscv_fitted: takes a fitted GridSearchCV object\n",
    "    grid: takes the grid used to fit the GridSearchCV object\n",
    "    \n",
    "    Returns:\n",
    "    top 20 configurations hyper-parameter presence graphic\n",
    "    (also outputs the \"logs\" of each grid-search to the output directory)\n",
    "    \"\"\"\n",
    "    # Saving parameter grid for specific Grid Search run\n",
    "    with open(\"./outputs/grids.txt\", \"a\") as file:\n",
    "        file.write(\"# {}------------------------------------------------------ #\\n\".format(id_num) + json.dumps(grid, cls=ComplexEncoder) + \"\\n\\n\")\n",
    "\n",
    "    # Saving cv_results for specific Grid Search run\n",
    "    score_summary = pd.DataFrame(gscv_fitted.cv_results_).sort_values(by=\"mean_test_score\", ascending=False)\n",
    "    score_summary.to_csv(\"./outputs/grid_search_results{}.csv\".format(id_num))\n",
    "    \n",
    "    # Assessing distribution of hyper-parameter values amongst top 20 models\n",
    "    sns.set()\n",
    "\n",
    "    # Features to plot\n",
    "    plot_features = list(map(lambda x: \"param_\" + x, {i for j in range(len(grid)) for i in grid[j].keys()}))\n",
    "    plot_df = score_summary.reset_index(drop=True).loc[:19, :].fillna(\"NaN\")\n",
    "\n",
    "    # figure and axes\n",
    "    fig, axes = plt.subplots(3, ceil(len(plot_features)/3), figsize=(23,13))\n",
    "\n",
    "    # plot data\n",
    "    for ax, x in zip(axes.flatten(), plot_features):\n",
    "        try:\n",
    "            sns.countplot(x=x, data=plot_df, ax=ax)\n",
    "        except TypeError:\n",
    "            sns.countplot(x=plot_df[x].apply(json.dumps), ax=ax)\n",
    "\n",
    "    plt.suptitle(\"Hyper-parameter presence on top 20 models\", y=0.95, fontsize=25)\n",
    "\n",
    "    plt.savefig(\"./outputs/grid_results{}.png\".format(id_num))\n",
    "    \n",
    "    # Model training - using best parameters to train model on entire data for submission\n",
    "    best_model = full_pipeline.set_params(**gscv_fitted.best_params_)\n",
    "    best_model.fit(X_train, y_train)  # Using all of the data to fit the model\n",
    "\n",
    "    # Predicting the \n",
    "    y_pred = best_model.predict(test_df)\n",
    "\n",
    "    # Submission\n",
    "    pd.DataFrame(data={\"Patient_ID\": test_df.index.to_list(), \"Deceased\": y_pred}).to_csv(\"./outputs/submission{}.csv\".format(id_num), index=False)\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def feature_importance(id_num, gscv_fitted, fimp_pre=True):\n",
    "    \"\"\"\n",
    "    Function that show feature importance plot and saves it to outputs folder\n",
    "    \n",
    "    id_num: takes an id_number which identifies the grid-search\n",
    "    gscv_fitted: takes a fitted GridSearchCV object\n",
    "    fimp_pre: whether to evaluate feature importance pre or pos model\n",
    "    \n",
    "    Returns:\n",
    "    feature importance graphic    \n",
    "    \"\"\"\n",
    "    if fimp_pre:\n",
    "        if (gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"]) and (gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"] != 'passthrough'):\n",
    "\n",
    "            # Get feature names\n",
    "            numeric_features = gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"join\"].transformers_[0][2]\n",
    "            categorical_features = gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"join\"].transformers_[1][2]\n",
    "            categorical_features = list(gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"join\"].named_transformers_[\"ohe\"].get_feature_names(categorical_features))\n",
    "            feature_names = numeric_features + categorical_features\n",
    "\n",
    "            # Get feature importances\n",
    "            if hasattr(gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"].estimator_, \"feature_importances_\"):  # Check if features_importances_ attribute exists. Necessary when there's tree selectors\n",
    "                if len(gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"].estimator_.feature_importances_.shape) != 2: \n",
    "                    feature_importances = pd.DataFrame(gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"].estimator_.feature_importances_, index=feature_names).T\n",
    "                else:\n",
    "                    feature_importances = pd.DataFrame(gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"].estimator_.feature_importances_, index=gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"].estimator_.classes_, columns=feature_names)\n",
    "            else:\n",
    "                if len(gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"].estimator_.coef_.shape) != 2: \n",
    "                    feature_importances = pd.DataFrame(gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"].estimator_.coef_, index=feature_names).T\n",
    "                else:\n",
    "                    feature_importances = pd.DataFrame(gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"].estimator_.coef_, index=gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"].estimator_.classes_, columns=feature_names)\n",
    "\n",
    "            colors = feature_importances.applymap(lambda x: \"tab:red\" if x < 0 else \"tab:blue\")  # Get positive and negative colors\n",
    "            threshold = gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"].threshold_  # Get SelectFromModel threshold value \n",
    "\n",
    "            sns.set()\n",
    "            if feature_importances.shape[0] == 3:\n",
    "                fig, axes = plt.subplots(3, 1, figsize=(23,12))\n",
    "                blue_patch = mpatches.Patch(color='tab:blue', label='Positive')\n",
    "                red_patch = mpatches.Patch(color='tab:red', label='Negative')\n",
    "                threshold_line = mlines.Line2D([], [], linestyle=\"dashed\", color='black', label='Threshold')\n",
    "\n",
    "                for i, ax in enumerate(axes.flatten()):\n",
    "                    ax.bar(x=feature_names, height=abs(feature_importances.iloc[i]), color=colors.iloc[i])\n",
    "                    ax.hlines(threshold, -0.5, 18.5, linestyles=\"dashed\")\n",
    "                    ax.set_xticklabels(feature_names, rotation = 30, ha=\"right\")\n",
    "                    ax.set_title(\"Class {} Feature Importance\".format(feature_importances.index[i]), fontsize=18)\n",
    "                    ax.legend(handles=[blue_patch, red_patch, threshold_line], fontsize=12)\n",
    "\n",
    "                plt.subplots_adjust(hspace=0.6)\n",
    "                plt.savefig(\"./outputs/feature_importance{}.png\".format(id_num))\n",
    "                return plt.show()\n",
    "            else: \n",
    "                fig = plt.figure(figsize=(23,6))\n",
    "                blue_patch = mpatches.Patch(color='tab:blue', label='Positive')\n",
    "                red_patch = mpatches.Patch(color='tab:red', label='Negative')\n",
    "                threshold_line = mlines.Line2D([], [], linestyle=\"dashed\", color='black', label='Threshold')\n",
    "\n",
    "                plt.bar(x=feature_names, height=abs(feature_importances.iloc[0]), color=colors.iloc[0])\n",
    "                plt.hlines(threshold, -0.5, len(feature_names) - 0.5, linestyles=\"dashed\")\n",
    "                plt.xticks(rotation=30, ha=\"right\")\n",
    "                plt.title(\"Pre Feature Importance\", fontsize=18)\n",
    "                plt.legend(handles=[blue_patch, red_patch, threshold_line], fontsize=12)\n",
    "\n",
    "                plt.savefig(\"./outputs/feature_importance_pre{}.png\".format(id_num))\n",
    "                return plt.show()\n",
    "        else:\n",
    "            print(\"No feature selection was done!\")\n",
    "    else:\n",
    "        if (gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"dimred\"]) and (gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"dimred\"] != 'passthrough'):\n",
    "            # Obtaining Component Names\n",
    "            feature_names = [\"PC\" + str(i) for i in range(1, best_model.named_steps[\"prep\"].named_steps[\"dimred\"].n_components + 1)]\n",
    "        else:  \n",
    "            # Get feature names\n",
    "            numeric_features = gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"join\"].transformers_[0][2]\n",
    "            categorical_features = gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"join\"].transformers_[1][2]\n",
    "            categorical_features = list(gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"join\"].named_transformers_[\"ohe\"].get_feature_names(categorical_features))\n",
    "            feature_names = numeric_features + categorical_features\n",
    "\n",
    "            # Get filtered feature names\n",
    "            if (gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"]) and (gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"] != 'passthrough'):\n",
    "                feature_names = np.array(feature_names)[gscv_fitted.best_estimator_.named_steps[\"prep\"].named_steps[\"fselect\"].get_support()]\n",
    "\n",
    "        # Get feature importances\n",
    "        if len(gscv_fitted.best_estimator_.named_steps[\"model\"].feature_importances_.shape) != 2: \n",
    "            feature_importances = pd.DataFrame(gscv_fitted.best_estimator_.named_steps[\"model\"].feature_importances_, index=feature_names).T\n",
    "        else:\n",
    "            feature_importances = pd.DataFrame(gscv_fitted.best_estimator_.named_steps[\"model\"].feature_importances_, index=gscv_fitted.best_estimator_.named_steps[\"model\"].classes_, columns=feature_names)\n",
    "        \n",
    "        colors = feature_importances.applymap(lambda x: \"tab:red\" if x < 0 else \"tab:blue\")  # Get positive and negative colors\n",
    "\n",
    "        sns.set()\n",
    "        fig = plt.figure(figsize=(23,6))\n",
    "        blue_patch = mpatches.Patch(color='tab:blue', label='Positive')\n",
    "        red_patch = mpatches.Patch(color='tab:red', label='Negative')\n",
    "\n",
    "        plt.bar(x=feature_names, height=abs(feature_importances.iloc[0]), color=colors.iloc[0])\n",
    "        plt.xticks(rotation=30, ha=\"right\")\n",
    "        plt.title(\"Pos Feature Importance\", fontsize=18)\n",
    "        plt.legend(handles=[blue_patch, red_patch], fontsize=12)\n",
    "\n",
    "        plt.savefig(\"./outputs/feature_importance_pos{}.png\".format(id_num))\n",
    "        return plt.show()\n",
    "        \n",
    "\n",
    "# def plot_tree(id_num, gscv_fitted):\n",
    "#     \"\"\"\n",
    "#     Function that shows the decision tree graph and saves it as png\n",
    "    \n",
    "#     id_num: takes an id_number which identifies the grid-search\n",
    "#     gscv_fitted: takes a fitted GridSearchCV object\n",
    "    \n",
    "#     Returns:\n",
    "#     decision tree graphic\n",
    "#     \"\"\"\n",
    "#     if (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect2\"]) and (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect2\"] != 'passthrough'):\n",
    "#         # Obtaining Component Names\n",
    "#         feature_names = [\"PC\" + str(i) for i in range(1, best_model.named_steps[\"prep\"].named_steps[\"fselect2\"].n_components + 1)]\n",
    "#     else:  \n",
    "#         # Get feature names\n",
    "#         numeric_features = gscv_fitted.named_steps[\"prep\"].named_steps[\"join_features\"].transformers_[0][2]\n",
    "#         categorical_features = gscv_fitted.named_steps[\"prep\"].named_steps[\"join_features\"].transformers_[1][2]\n",
    "#         categorical_features = list(gscv_fitted.named_steps[\"prep\"].named_steps[\"join_features\"].named_transformers_[\"categorical_pipeline\"].named_steps[\"one_hot_encoder\"].get_feature_names(categorical_features))\n",
    "#         feature_names = metric_features + categorical_features\n",
    "\n",
    "#         # Get filtered feature names\n",
    "#         if (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"]) and (gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"] != 'passthrough'):\n",
    "#             feature_names = np.array(feature_names)[gscv_fitted.named_steps[\"prep\"].named_steps[\"fselect1\"].get_support()]\n",
    "    \n",
    "#     # Set PATH variable to locate graphviz executables\n",
    "#     if os.environ['PATH'].split(\";\")[-1] != 'C:\\\\Users\\\\davids\\\\Anaconda3\\\\envs\\\\ml\\\\Library\\\\bin\\\\graphviz':\n",
    "#         os.environ['PATH'] = os.environ['PATH'] + ';' + os.environ['CONDA_PREFIX'] + r\"\\Library\\bin\\graphviz\"\n",
    "    \n",
    "#     # Export a decision tree in DOT format\n",
    "#     dot_data = export_graphviz(gscv_fitted.named_steps[\"model\"],\n",
    "#                                feature_names=feature_names,  \n",
    "#                                class_names=gscv_fitted.named_steps[\"model\"].classes_,\n",
    "#                                filled=True,\n",
    "#                                rounded=True)\n",
    "    \n",
    "#     pydot_graph = pydotplus.graph_from_dot_data(dot_data)  # load graph in dot format\n",
    "#     pydot_graph.set_size('\"50,50\"')  # set size of graph figure\n",
    "#     pydot_graph.write_png(\"./outputs/decision_tree{}.png\".format(id_num))  # output graph as png\n",
    "#     return graphviz.Source(pydot_graph.to_string())\n",
    "\n",
    "\n",
    "# def effective_alpha_analysis(id_num, gscv_fitted, X_train, y_train, max_alpha, min_alpha=0):\n",
    "#     \"\"\"\n",
    "#     Function that analyses the alphas of a decision tree and plots the cross-validated mean alpha performance of default trees.\n",
    "    \n",
    "#     id_num: takes an id_number which identifies the grid-search\n",
    "#     gscv_fitted: takes a fitted GridSearchCV object\n",
    "#     X_train: raw train data\n",
    "#     y_train: train data labels\n",
    "#     max_alpha: max alpha consider in cross-validation\n",
    "#     min_alpha: min alpha consider in cross-validation\n",
    "    \n",
    "#     Returns:\n",
    "#     alpha analysis graphic\n",
    "#     \"\"\"\n",
    "#     original_model = clone(gscv_fitted.named_steps[\"model\"])\n",
    "#     analyze_model = clone(gscv_fitted.named_steps[\"model\"].set_params(min_weight_fraction_leaf=0.0, max_depth=None))\n",
    "\n",
    "#     # Transforming X_train based on fitted preparation pipeline\n",
    "#     X_train_trans = gscv_fitted.named_steps[\"prep\"].transform(X_train)\n",
    "\n",
    "#     # Producing cost_complexity_pruning_path of default tree on X_train_trans\n",
    "#     path = analyze_model.cost_complexity_pruning_path(X_train_trans, y_train)\n",
    "#     ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "#     # Defininf a default tree for each ccp_alpha value found\n",
    "#     ccp_alphas_delim = ccp_alphas[(ccp_alphas >= min_alpha) & (ccp_alphas <= max_alpha)]\n",
    "#     trees = [clone(analyze_model.set_params(ccp_alpha=ccp_alpha)) for ccp_alpha in ccp_alphas_delim]\n",
    "\n",
    "#     # Producing 5-fold cv f1-scores for each tree on X_train_trans\n",
    "#     cv_scores = np.array([cross_val_score(tree, X_train_trans, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=1), scoring='f1_micro', n_jobs=-1) for tree in trees])\n",
    "#     cv_scores_mean, cv_scores_std = cv_scores.mean(axis=1), cv_scores.std(axis=1)\n",
    "#     cv_scores_ci95 = 1.96 * cv_scores_std / cv_scores_mean\n",
    "#     global_optimum = [ccp_alphas_delim[np.argmax(cv_scores_mean)], np.max(cv_scores_mean)]\n",
    "#     original_cv_score = cross_val_score(original_model, X_train_trans, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=1), scoring='f1_micro', n_jobs=-1).mean()\n",
    "    \n",
    "#     sns.set()\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(23, 8))\n",
    "\n",
    "#     # Plot Total Impurity vs effective alpha for training set\n",
    "#     ax1.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "#     ax1.set_xlabel(\"effective alpha\", fontsize=14)\n",
    "#     ax1.set_ylabel(\"total impurity of leaves\", fontsize=14)\n",
    "#     ax1.set_title(\"Total Impurity vs effective alpha for training set\", fontsize=18)\n",
    "\n",
    "#     # Plot F1-score vs alpha for cross-validation score\n",
    "#     ax2.plot(ccp_alphas_delim, cv_scores_mean, marker='o', label=\"AVG Scores\", drawstyle=\"steps-post\")\n",
    "#     ax2.plot(global_optimum[0], global_optimum[1], 'o', label=\"G.O.: [{0:.6f}, {1:.3f}]\".format(global_optimum[0], global_optimum[1]))\n",
    "#     ax2.fill_between(ccp_alphas_delim, (cv_scores_mean - cv_scores_ci95), (cv_scores_mean + cv_scores_ci95), color='b', alpha=.1)\n",
    "#     ax2.hlines(original_cv_score, min_alpha, max_alpha, linestyles=\"dashed\", label=\"Original Model\")\n",
    "#     ax2.set_xlim(left=min_alpha, right=max_alpha)\n",
    "#     ax2.set_xlabel(\"alpha\", fontsize=14)\n",
    "#     ax2.set_ylabel(\"f1-score\", fontsize=14)\n",
    "#     ax2.set_title(\"F1-score vs alpha for cross-validation score\", fontsize=18)\n",
    "#     ax2.legend()\n",
    "    \n",
    "#     plt.savefig(\"./outputs/alpha_analysis{}.png\".format(id_num))\n",
    "#     return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell holds all the Custom Transformers we designed to pipeline the raw data to the model\n",
    "class PrepImpute(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Creates Title\n",
    "    Creates Title_binary\n",
    "    Fills Medical_Tent with \"NK\"\n",
    "    Imputes City with most frequent value\n",
    "    \"\"\"\n",
    "    #Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.city_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        \n",
    "    #Return self nothing else to do here\n",
    "    def fit(self, X, y=None):\n",
    "        self.city_imputer.fit(X[[\"City\"]])\n",
    "        return self\n",
    "    \n",
    "    #Transformer method we wrote for this transformer \n",
    "    def transform(self, X):\n",
    "        \n",
    "        X2 = X.copy()\n",
    "        \n",
    "        # CREATE NEW VARIABLES\n",
    "        X2['Title'] = X2['Name'].str.split('\\\\W', 1, expand=True)[0]\n",
    "        X2['Title_binary'] = X2['Title'].apply(lambda x: 1 if x in [\"Master\",\"Miss\"] else 0)\n",
    "        X2[\"Medical_Tent\"] = X2[\"Medical_Tent\"].fillna(value=\"NK\")\n",
    "        X2['City'] = self.city_imputer.transform(X2[[\"City\"]])\n",
    "        \n",
    "        return X2 \n",
    "    \n",
    "\n",
    "class KNNImputerScaled(KNNImputer):\n",
    "    \"\"\"\n",
    "    KNNImputer subclass that scales variables before applying imputation\n",
    "    Impute Birthday_year using scaled and non-scaled variables with KNNImputer\n",
    "    \"\"\"\n",
    "    def __init__(self, missing_values=np.nan, n_neighbors=5, weights='uniform', metric='nan_euclidean', copy=True, add_indicator=False,\n",
    "                 columns_to_norm=['Severity','Parents_Children_Infected','Partner_Siblings_Infected'], columns_not_to_norm=['Birthday_year','Title_binary'],\n",
    "                 scaler=MinMaxScaler(), **scaler_args):\n",
    "        super().__init__(missing_values=missing_values, \n",
    "                         n_neighbors=n_neighbors, \n",
    "                         weights=weights,\n",
    "                         metric=metric,\n",
    "                         copy=copy, \n",
    "                         add_indicator=add_indicator)\n",
    "        self.scaler = scaler.set_params(**scaler_args)\n",
    "        self.columns_to_norm = columns_to_norm\n",
    "        self.columns_not_to_norm = columns_not_to_norm\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X_scaled = self.scaler.fit_transform(X[self.columns_to_norm])\n",
    "        X_trans = np.concatenate([X_scaled, X[self.columns_not_to_norm]], axis=1)\n",
    "        super().fit(X_trans)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_scaled = self.scaler.transform(X[self.columns_to_norm])\n",
    "        X_trans = np.concatenate([X_scaled, X[self.columns_not_to_norm]], axis=1)\n",
    "        X_imputed_scaled = pd.DataFrame(super().transform(X_trans), index=X.index, columns=self.columns_to_norm + self.columns_not_to_norm)\n",
    "        X_imputed = X.copy()\n",
    "        X_imputed[\"Birthday_year\"] = X_imputed_scaled[\"Birthday_year\"].round(0).astype(int)\n",
    "        \n",
    "        return X_imputed\n",
    "    \n",
    "    \n",
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Creates Age\n",
    "    Creates Gender\n",
    "    Ceates Parents_Children_Infected_Binary\n",
    "    Creates Partner_Siblings_Infected_Binary\n",
    "    Creates Medical_Tent_Binary\n",
    "    Creates Family_Infected\n",
    "    Creates Family_Infected_Binary\n",
    "    Creates Family_Deceased\n",
    "    Creates Family_Deceased_Reduced\n",
    "    Creates Medical_Expenses_Individual\n",
    "    \"\"\"\n",
    "    #Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        pass    \n",
    "        \n",
    "    #Return self nothing else to do here\n",
    "    def fit(self, X, y):\n",
    "        self.familycount_ = X.Family_Case_ID.value_counts().to_dict()\n",
    "        X_train_w_dec = pd.concat([X, y], axis=1)\n",
    "        self.deceasedfamcount_ = X_train_w_dec.loc[X_train_w_dec['Deceased']==1].Family_Case_ID.value_counts().to_dict()\n",
    "        return self\n",
    "    \n",
    "    #Transformer method we wrote for this transformer \n",
    "    def transform(self, X):\n",
    "        X2 = X.copy()\n",
    "        # CREATE NEW VARIABLES\n",
    "        X2['Age'] = X2['Birthday_year'].map(lambda x: 2020 - x)\n",
    "        X2['Gender'] = X2['Title'].map(lambda x: \"Male\" if x in [\"Mr\", \"Master\"] else \"Female\")\n",
    "        X2['Parents_Children_Infected_Binary'] = X2['Parents_Children_Infected'].map(lambda x: 0 if x==0 else 1)  \n",
    "        X2['Partner_Siblings_Infected_Binary'] = X2['Partner_Siblings_Infected'].map(lambda x: 0 if x==0 else 1)\n",
    "        X2[\"Medical_Tent_Binary\"] = X2[\"Medical_Tent\"].map(lambda x: x if x == \"NK\" else \"K\")\n",
    "        #X2[\"Pediatric_Binary\"] = X2[\"Age\"].map(lambda x: 1 if x < 18 else 0)\n",
    "        #X2[\"3rd_Age_Binary\"] = X2[\"Age\"].map(lambda x: 1 if x >= 65 else 0)\n",
    "        X2[\"Family_Infected\"] = X2[\"Family_Case_ID\"].map(self.familycount_).fillna(1)\n",
    "        X2[\"Family_Infected_Binary\"] = X2[\"Family_Infected\"].map(lambda x: 0 if x<=1 else 1)\n",
    "        X2[\"Family_Deceased\"] = (X2[\"Family_Case_ID\"].map(self.deceasedfamcount_) - 1).fillna(0)\n",
    "        X2[\"Family_Deceased_Reduced\"] = X2['Family_Deceased'].map(lambda x: x if x==0 else (2 if x>=3 else 1))\n",
    "        #X2[\"Dead_infected_ratio_family\"] = X2[\"Family_Deceased\"] / X2[\"Family_Infected\"]\n",
    "        X2[\"Medical_Expenses_Individual\"] = X2[\"Medical_Expenses_Family\"] / X2[\"Family_Infected\"]\n",
    "        # FURTHER TRANSFORMATIONS\n",
    "        X2[\"Medical_Expenses_Individual\"][X2[\"Medical_Expenses_Individual\"] > 3000] = 3000\n",
    "        \n",
    "        return X2 \n",
    "    \n",
    "    \n",
    "class Transformations(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transforms Medical_Expenses_Individual by applying Box-Cox power transformation\n",
    "    Drops unnecessary variables\n",
    "    \"\"\"\n",
    "    #Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def get_alpha(self):\n",
    "        return self.alpha_\n",
    "        \n",
    "    #Return self nothing else to do here\n",
    "    def fit(self, X, y=None):\n",
    "        self.alpha_ = boxcox(X[\"Medical_Expenses_Individual\"] + 1)[1]\n",
    "        return self\n",
    "\n",
    "    #Transformer method we wrote for this transformer \n",
    "    def transform(self, X):\n",
    "        X2 = X.copy()\n",
    "        X2[\"Medical_Expenses_Individual\"] = boxcox(X2[\"Medical_Expenses_Individual\"] + 1, alpha=self.alpha_)[0]\n",
    "        # DROP INTERMEDIATE VARIABLES\n",
    "        X2 = X2.drop([\"Family_Case_ID\", \"Name\", \"Birthday_year\", \"Title\", \"Title_binary\", \"Partner_Siblings_Infected\", \n",
    "                    \"Medical_Tent\", \"Parents_Children_Infected\", \"Partner_Siblings_Infected_Binary\", \n",
    "                    \"Parents_Children_Infected_Binary\", \"Family_Deceased\", \"Family_Infected\", \"Medical_Expenses_Family\"], axis=1)\n",
    "        \n",
    "        return X2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pipe = train_df.drop('Deceased', axis=1)\n",
    "y_train_pipe = train_df['Deceased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Pipeline\n",
    "clean_pipeline = Pipeline([\n",
    "    ('prepimpute', PrepImpute()),\n",
    "    ('knnimputer', KNNImputerScaled()),  # tune: n_neighbors, weights, scaler\n",
    "    ('engineering', FeatureEngineering()),\n",
    "    ('transformation', Transformations())\n",
    "])\n",
    "\n",
    "# Spliting features\n",
    "scale_columns = [\"Severity\", \"Age\", \"Family_Deceased_Reduced\", \"Medical_Expenses_Individual\"]\n",
    "ohe_columns = [\"City\", \"Gender\", \"Medical_Tent_Binary\", \"Family_Infected_Binary\"]\n",
    "\n",
    "# Combining features and applying different transformations\n",
    "join_pipeline = ColumnTransformer([('scaler', \"passthrough\", scale_columns),\n",
    "                                   ('ohe', OneHotEncoder(sparse=False), ohe_columns)])  # tune: drop\n",
    "\n",
    "# Feature Selection - SelectKBest, ExtraTreesRegressor, Elasticnet\n",
    "fselect1 = SelectFromModel(LogisticRegression(penalty=\"elasticnet\", max_iter=400, solver=\"saga\", n_jobs=-1, random_state=1))  # tune: estimator__C, l1_ratio=0 (l2) l1_ratio=1 (l1)\n",
    "fselect2 = SelectFromModel(ExtraTreesClassifier(n_estimators=20, n_jobs=-1, random_state=1))\n",
    "fselect3 = SelectKBest()  # tune: k\n",
    "\n",
    "# Dimensionality Reduction - NeighborhoodComponentsAnalysis, PCA\n",
    "dimred1 = NeighborhoodComponentsAnalysis(max_iter=25, tol=0.005, random_state=1)  # tune: n_components, tol=0.005 (it takes to long)\n",
    "dimred2 = PCA(random_state=1)\n",
    "\n",
    "# Full Preprocessing Pipeline\n",
    "prep_pipeline = Pipeline([\n",
    "    (\"clean\", clean_pipeline),\n",
    "    (\"join\", join_pipeline),\n",
    "    (\"fselect\", \"passthrough\"),\n",
    "    (\"dimred\", \"passthrough\")\n",
    "])\n",
    "\n",
    "# Models - Decision Trees, Random Forests, XG Boost, MLP, KNN, Logistic Regression (Stackings, Bagging, Keras NN)\n",
    "lr = LogisticRegression(max_iter=400, random_state=1, n_jobs=-1)\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier(random_state=1, n_jobs=-1)\n",
    "mlp = MLPClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "# xgb =  # XG Boost\n",
    "\n",
    "# Full Model Pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    (\"prep\", prep_pipeline),\n",
    "    (\"model\", \"passthrough\")\n",
    "])\n",
    "\n",
    "grid = [\n",
    "#     {\"prep__clean__knnimputer__n_neighbors\": [5, 15],\n",
    "#      \"prep__clean__knnimputer__weights\": [\"distance\"],\n",
    "#      \"prep__clean__knnimputer__scaler\": [MinMaxScaler()],\n",
    "#      \"prep__join__scaler\": [None, MinMaxScaler()],\n",
    "#      \"prep__join__ohe__drop\": [None, \"first\"],\n",
    "#      \"prep__fselect\": [None, fselect2],\n",
    "#      \"prep__dimred\": [None],\n",
    "#      \"model\": [rf],\n",
    "#      \"model__n_estimators\": [150, 200, 250],\n",
    "#      \"model__criterion\": ['gini', 'entropy'],\n",
    "#      \"model__max_depth\": [None, 10, 20],\n",
    "#      \"model__max_features\": [\"sqrt\"],\n",
    "#      \"model__ccp_alpha\": [0, 0.01, 0.015, 0.02],\n",
    "#      \"model__max_samples\": [None, 0.5]\n",
    "#     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'model': RandomForestClassifier(bootstrap=True, ccp_alpha=0.015, class_weight=None,\n",
    "                       criterion='entropy', max_depth=None, max_features='sqrt',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
    "                       n_jobs=-1, oob_score=False, random_state=1, verbose=0,\n",
    "                       warm_start=False), 'model__ccp_alpha': 0.02, 'model__criterion': 'entropy', 'model__max_depth': None, 'model__max_features': 'sqrt', 'model__max_samples': 0.5, 'model__n_estimators': 200, 'prep__clean__knnimputer__n_neighbors': 15, 'prep__clean__knnimputer__scaler': MinMaxScaler(copy=True, feature_range=(0, 1)), 'prep__clean__knnimputer__weights': 'distance', 'prep__dimred': None, 'prep__fselect': SelectFromModel(estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,\n",
    "                                               class_weight=None,\n",
    "                                               criterion='gini', max_depth=None,\n",
    "                                               max_features='auto',\n",
    "                                               max_leaf_nodes=None,\n",
    "                                               max_samples=None,\n",
    "                                               min_impurity_decrease=0.0,\n",
    "                                               min_impurity_split=None,\n",
    "                                               min_samples_leaf=1,\n",
    "                                               min_samples_split=2,\n",
    "                                               min_weight_fraction_leaf=0.0,\n",
    "                                               n_estimators=20, n_jobs=-1,\n",
    "                                               oob_score=False, random_state=1,\n",
    "                                               verbose=0, warm_start=False),\n",
    "                max_features=None, norm_order=1, prefit=False, threshold=None), 'prep__join__ohe__drop': None, 'prep__join__scaler': MinMaxScaler(copy=True, feature_range=(0, 1))}\n",
    "\n",
    "# Model training - using best parameters to train model on entire data for submission\n",
    "best_model = full_pipeline.set_params(**params)\n",
    "best_model.fit(X_train, y_train)  # Using all of the data to fit the model\n",
    "\n",
    "# Predicting the \n",
    "y_pred = best_model.predict(test_df)\n",
    "\n",
    "# Submission\n",
    "pd.DataFrame(data={\"Patient_ID\": test_df.index.to_list(), \"Deceased\": y_pred}).to_csv(\"./outputs/submission{}.csv\".format(6_3), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiating GridSearch\n",
    "gscv = GridSearchCV(full_pipeline, grid, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=1), scoring='accuracy', return_train_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Get ID of grid search\n",
    "id_num = input(\"Insert GridSearch ID number: \")\n",
    "\n",
    "# Grid Search and model training\n",
    "gscv.fit(X_train_pipe, y_train_pipe)\n",
    "\n",
    "# Best params and best score\n",
    "print(\"\\nBest score: {} \\nBest parameters: {}\".format(gscv.best_score_, gscv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain outputs from Grid Search\n",
    "gs_outputs(id_num, gscv, grid)  # if error occurs check ComplexEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance pre\n",
    "feature_importance(id_num, gscv, fimp_pre=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance pos\n",
    "feature_importance(id_num, gscv, fimp_pre=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
